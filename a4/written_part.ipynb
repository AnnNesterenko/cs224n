{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translations with RNN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tasks are code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing NMT systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "Cherokee is a polysynthetic language. It means that words are composed from many morphemes and these morphemes have independent meaning. A single word in Cherokee language can be the whole sentence in English. \n",
    "\n",
    "It is important to model Cherokee-English NMT at the subword level, because subword in Cherokee is the independent word in English with independent meaning. If model used the whole word in Cherokee, thanslation would be worth because there is no single English word that fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "Word embeddings represent the meaning and semantic relations of words, their size is usuallly proportional to the size of the vocabulary. With the huge size of the vocabulary and only few frequent words, the vectorization matrices would be enormous, but with mostly zeros.\n",
    "\n",
    "The number of characters is very limited (English - 26 lower case letter and 26 upper case ones), so even with one-hot encoding the embeddings would be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)**\n",
    "\n",
    "Multilingual training uses transfer learning,  they are effective at generalization, and capable of capturing the representational similarity across a large body of languages. Insights gained through training on one language can be applied to the translation of other languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)**\n",
    "\n",
    "(i) **Source Translation**: Yona utsesdo ustiyegv anitsilvsgi digvtanv uwoduisdei. \n",
    "\n",
    "**Reference Translation**: Fern had a crown of daisies in her hair.\n",
    "\n",
    "**NMT Translation**: Fern had `her hair` with her hair.\n",
    "\n",
    "\n",
    "*Reason*: probably there isn't enough samples in training data with pair `a crown of daisies` - `anitsilvsgi digvtanv`. NMT might use 'greedy decoding', it can't find good translation for `anitsilvsgi digvtanv` and copies 'her hair' from the back of the sentence.\n",
    "\n",
    "*Fix*: maybe beam search\n",
    "\n",
    "(ii) **Source Sentence**: Ulihelisdi nigalisda. \n",
    "\n",
    "**Reference Translation**: She is very excited. \n",
    "\n",
    "**NMT Translation:** `It’s` joy.\n",
    "\n",
    "*Reason*: I think it is the lack of training data or maybe epochs. NMT confused it and she.\n",
    "\n",
    "*Fix*: add more epoch or training data\n",
    "\n",
    "(iii) ***Source Sentence***: Tsesdi hana yitsadawoesdi usdi atsadi! \n",
    "\n",
    "**Reference Translation**: Don’t swim there, Littlefish!\n",
    "\n",
    "**NMT Translation:** Don’t know how a small fish!\n",
    "\n",
    "*Reason:* NMT translation is sequential and literal\n",
    "\n",
    "*Fix*: add to training data more examples with figurative meaning and proper names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C1** = the love can always do\n",
    "<img src='images/1.png' width=300>\n",
    "\n",
    "$p_1 = 0.6, p_2 = 0.5, len(c) = 5, len(r) = 4, BP = 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5477225575051662"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLEU_c1 = exp(0.5 * log(0.6) + 0.5 * log(0.5))\n",
    "BLEU_c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C2** = love can make anything possible\n",
    "\n",
    "<img src='images/2.png' width=300>\n",
    "\n",
    "$p_1 = 0.8, p_2 = 0.5, len(c) = 5, len(r) = 4, BP = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6324555320336759"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLEU_c2 = exp(0.5 * log(0.8) + 0.5 * log(0.5))\n",
    "BLEU_c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to BLEU c2 is better translation, I agree with this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii\n",
    "\n",
    "Assuming that only r1 reference translation is avaliable:\n",
    "\n",
    "**C1**\n",
    "<img src='images/3.png' width=300>\n",
    "\n",
    "$p_1 = 0.6, p_2 = 0.5, len(c) = 5, len(r) = 4, BP = e^{-0.2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4484373019840029"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLEU_c1_1 = exp(-0.2) * exp(0.5 * log(0.6) + 0.5 * log(0.5))\n",
    "BLEU_c1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/4.png' width=300>\n",
    "\n",
    "$p_1 = 0.4, p_2 = 0.25, len(c) = 5, len(r) = 4, BP = e^{-0.2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2589053970151336"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLEU_c2 = exp(-0.2) * exp(0.5 * log(0.4) + 0.5 * log(0.25))\n",
    "BLEU_c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to BLEU c1 is better translation, I disagree with this choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii\n",
    "\n",
    "In some languages the order of words in sentence is undeterminate. BLEU consider only consecutive and overlapping n-gramms. A well-chosen translation, but with different word order, may lead to low BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv\n",
    "\n",
    "**Advantages of BLEU**\n",
    "\n",
    "- faster than human\n",
    "- objective numeric metric, you can definetely say which thanslation is better\n",
    "- simple implementation (people need to know both languages)\n",
    "\n",
    "\n",
    "**Disadvantages of BLEU**\n",
    "\n",
    "- doesn't measures semantical, logical and grammatical correctness\n",
    "- considers only groups of consecutive words and doesn't estimate sentence in whole "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
