{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)**\n",
    "\n",
    "$$- \\sum_{w \\in Vocab} y_w log(\\hat{y}_w) = - 0 \\cdot log(\\hat{y}_0) - 0 \\cdot log(\\hat{y}_1) - \\ldots - 1 \\cdot log(\\hat{y}_o) - \\ldots - 0 \\cdot log(\\hat{y}_{len(Vocab)}) = - log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "\n",
    "From the above we know that \n",
    "$$J = J_{naive\\_softmax} = CrossEntropy(y, \\hat{y})$$\n",
    "\n",
    "From http://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf (7):\n",
    "$$\\frac{\\partial{J}}{\\partial{\\theta}} = (\\hat{y} - y)^T, \\text{ where } \\hat{y} = softmax(\\theta)$$\n",
    "\n",
    "Let be $\\theta = U v_c$. Then\n",
    "$$\\frac{\\partial{J}}{\\partial{v_c}} = \\frac{\\partial{J}}{\\partial{\\theta}} \\frac{\\partial{\\theta}}{\\partial{v_c}} = (\\hat{y} - y)^T U$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) + (d)**\n",
    "\n",
    "The same way as above:\n",
    "$$\\frac{\\partial{J}}{\\partial{U}} = \\frac{\\partial{J}}{\\partial{\\theta}} \\frac{\\partial{\\theta}}{\\partial{U}} = (\\hat{y} - y) v_c$$\n",
    "\n",
    "The dimensionality of $\\frac{\\partial{J}}{\\partial{U}}$ is the same as of $U$, so \n",
    "$$\\frac{\\partial{J}}{\\partial{u_w}} - \\text{ the $w$-th row of } \\frac{\\partial{J}}{\\partial{U}}$$\n",
    "\n",
    "In terms of partial derivatives for $u_1, u_2, \\ldots, u_{|vocab|}$:\n",
    "$$\\frac{\\partial{J}}{\\partial{U}} = \\left( \\frac{\\partial{J}}{\\partial{u_1}}, \\frac{\\partial{J}}{\\partial{u_2}}, \\ldots, \\frac{\\partial{J}}{\\partial{u_{|vocab|}}} \\right)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)**:\n",
    "\n",
    "$$\\sigma(x) = \\frac{e^x}{e^x + 1}$$\n",
    "\n",
    "$$\\frac{\\partial{\\sigma(x)}}{\\partial{x}} = \\frac{\\partial{\\sigma(x)}}{\\partial{e^x}} \\frac{\\partial{e^x}}{\\partial{x}} = \\left(- \\frac{e^x}{(1+e^x)^2} + \\frac{1}{1 + e^x}\\right)e^x = \\sigma(x) - \\sigma(x)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)**:\n",
    "\n",
    "From this place let \n",
    "$$J = J_{neg-sample}(v_c, o, U) = - log(\\sigma(u_o^T v_c)) - \\sum\\limits_{k=1}^K log(\\sigma(-u_k^T v_c))$$\n",
    "\n",
    "1. Partial derivate of $J$ with respect to $v_c$:\n",
    "\n",
    "    $$\\frac{\\partial{J}}{\\partial{v_c}} = - \\frac{\\partial{log(\\sigma(u_o^T v_c))}}{\\partial{v_c}} - \\sum\\limits_{k=1}^K \\frac{\\partial{log(\\sigma(- u_k^T v_c))}}{\\partial{v_c}}$$\n",
    "\n",
    "    $$\\frac{\\partial{log(\\sigma(u_k^T v_c))}}{\\partial{v_c}} = (1 - \\sigma(u_k^Tv_c))u_k$$\n",
    "\n",
    "    $$\\frac{\\partial{J}}{\\partial{v_c}} = (\\sigma(u_o^Tv_c) - 1)u_o + \\sum\\limits_{k=1}^K (1 - \\sigma(- u_k^Tv_c))u_k$$\n",
    "    \n",
    "    \n",
    "2. Partial derivate of $J$ with respect to $u_o$:\n",
    "    $$\\frac{\\partial{J}}{\\partial{u_o}} = (\\sigma(u_o^Tv_c) - 1)v_c$$\n",
    "    \n",
    "3. Partial derivate of $J$ with respect to $u_k$:\n",
    "    $$\\frac{\\partial{J}}{\\partial{u_k}} = (1 - \\sigma(- u_k^Tv_c))v_c$$\n",
    "    \n",
    "    \n",
    "Naive softmax loss computes with the whole U while negative sampling loss uses only K samples. That's why this loss is more time and memory efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)**:\n",
    "\n",
    "Assuming that K sampled words aren't distinct:\n",
    "$$\\frac{\\partial{J}}{\\partial{u_o}} = m(1 - \\sigma(- u_k^Tv_c))v_c, \\text{ where } m \\text{ is the number of times } w_k \\text{occurs in } \\{w_1, \\ldots, w_K\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)**:\n",
    "\n",
    "$$\\frac{\\partial{J_{skip-gramm} (v_c, w_{t-m}, \\ldots, w_{t+m}, U)}}{\\partial{U}} = \\sum\\limits_{-m \\leq j \\leq m, j \\neq 0} \\frac{\\partial{J_{skip-gramm} (v_c, w_{t+j}, U)}}{\\partial{U}}$$\n",
    "\n",
    "$$\\frac{\\partial{J_{skip-gramm} (v_c, w_{t-m}, \\ldots, w_{t+m}, U)}}{\\partial{v_c}} = \\sum\\limits_{-m \\leq j \\leq m, j \\neq 0} \\frac{\\partial{J_{skip-gramm} (v_c, w_{t+j}, U)}}{\\partial{v_c}}$$\n",
    "\n",
    "$$\\frac{\\partial{J_{skip-gramm} (v_c, w_{t-m}, \\ldots, w_{t+m}, U)}}{\\partial{v_w}} = 0, \\text{ where } w \\neq c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word_vectors.png**\n",
    "\n",
    "<img src='word_vectors.png'>\n",
    "\n",
    "At the plot we can see that some word vectors and their relative location correlates with reality, but some look strange. \n",
    "For example, vectors 'male-female' and 'king-queen' are of the similar length and look parallel. Words of the same context such as 'tea' and 'coffee', 'woman' and 'female' are nearby. However, I can't explain why 'boring' is so close to 'amazing', 'great' and 'wonderful' or why 'man' is closer to 'female' that to 'male'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
